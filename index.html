    <!doctype html>
<html lang="en">

<head>
  
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BVKM9SY0E2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BVKM9SY0E2');
</script>


  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha512-xh6O/CkQoPOWDdYTDqeRdPCVd1SpvCA9XXcUnZS2FmJNp1coAFzvtCN9BmamE+4aHK8yyUHUSCcJHgXloTyT2A==" crossorigin="anonymous" referrerpolicy="no-referrer" />

  <title>Robotics Engineer</title>
  <link rel="icon" type="image/x-icon" href="assets/emoji_1.png">


  <link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet">
    <style>
    html {
      scroll-behavior: smooth;
    }

      body {
        margin: 0;
        font-family: 'Merriweather', serif;
        font-size: 15px;
        line-height: 1.6;
        color: #333;
      }

      h1 {
      font-size: 24px;
      font-weight: bold;
      border-bottom: 2px solid #000;
      display: inline-block;
    }
    h2 {
      font-size: 20px;
      font-weight: bold;
      margin-top: 10px;
    }
    .h2_nik {
      font-size: 20px;
      font-weight: bold;
      margin-top: 30px;
    }
    ul {
      margin-top: 0;
      margin-bottom: 10px;
    }
    li {
      margin-bottom: 4px;
    }
    .li_nik {
      margin-bottom: 8px;
      margin-top: 4px;
    }
    .degree {
      font-weight: bold;
    }
    .school {
      font-style: italic;
    }
    .date {
      float: right;
    } 
.header {
  display: flex;
  align-items: center;
  justify-content: center;
  background-color: #eee;
  padding: 1rem;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  z-index: 9999;
}

.header__logo {
  font-weight: bold;
  font-size: 1.5rem;
  margin-right: auto;
}

.header__nav {
  display: flex;
  align-items: center;
}

.header__nav__list {
  list-style: none;
  margin: 0;
  padding: 0;
  display: flex;
  justify-content: center;
  margin-left: auto;
}

.header__nav__item {
  margin-left: 1.5rem;
}

.header__nav__item:first-child {
  margin-left: 0;
}

.header__nav__item a {
  text-decoration: none;
  color: #333;
  font-weight: bold;
}

      .gpa {
        font-weight: bold;
        margin-top: 5px;
      }
      .experience {
      background-color: #f8f8f8;
      padding: 80px 0;
      }

      .experience__item {
        margin-bottom: 30px;
      }

        .experience__item__title {
          margin: 0;
          font-size: 24px;
          font-weight: bold;
        }

        .experience__item__meta {
          margin: 10px 0;
          font-size: 14px;
        }

        .experience__item__meta__company {
          margin-right: 10px;
        }

        .experience__item__meta__location {
          margin-right: 10px;
        }

        .experience__item__meta__date {
          margin-right: 10px;
        }

        .experience__item__responsibilities {
          margin: 0;
          padding: 0 0 0 20px;
        }

        .experience__item__responsibilities li {
          margin-bottom: 10px;
          font-size: 16px;
        }

        .competitions {
          margin-top: 40px;
        }

        .competitions__list {
          list-style: none;
          padding: 0;
          margin: 0;
        }

        .competitions__item {
          margin-bottom: 20px;
        }

        .competitions__title {
          margin-bottom: 10px;
          font-size: 24px;
        }

        .competitions__title_award {
          margin-bottom: 10px;
          font-size: 18px;
        }

        .competitions__description {
          margin-bottom: 10px;
          font-size: 15px;
        }

        .competitions__result_red {
          font-style: italic;
          color: #FF0000;
        }
        .end {
          font-size: 15px;
        }
        
        @media (max-width: 767px) {
			.header {
				display: none;
			}

      </style>
</head>

<body>
    <header class="header">
  <div class="header__logo">Nigam Katta</div>
  <nav class="header__nav">
    <ul class="header__nav__list">
      <li class="header__nav__item"><a href="#about-me" class="scroll-to">About Me</a></li>
      <li class="header__nav__item"><a href="#experience">Experience</a></li>
      <li class="header__nav__item"><a href="#skills">Skills</a></li>
      <li class="header__nav__item"><a href="#projects">Projects</a></li>
      <li class="header__nav__item"><a href="#publications">Publications</a></li>
      <li class="header__nav__item"><a href="#competitions">Competitions</a></li>
      <li class="header__nav__item"><a href="#awards">Awards</a></li>
    </ul>
  </nav>
</header>


    <div class="container">
        <div class="row" style="margin-top: 5em; ">
            <div class="col-sm-12" style="margin-bottom: 1em;" id="about-me">
            <h3 class="display-4" style="text-align: center;"><span style="font-weight: bold;">Nigam </span>Katta</h3>
            </div>
            <br>
            
            <div class="col-md-8" style="">
                
                <p>
                    <span style="font-weight: bold;">Actively looking for Full-Time Roles</span> 
                    
                </p>

                <p>
                    <span style="font-weight: bold;">Bio:</span> 
                    I am currently pursuing my Master's degree in Robotics at <a href="https://www.gatech.edu/" target="_blank">Georgia Institute of Technology</a>, starting from Fall 2022. My current research lies in the field of Object Detection, Localization, tracking and Path generation Algorithms. 
                </p>

                <p>
                    Prior to starting my Master's, I worked as a ADAS Software Engineer at <a href="https://multicorewareinc.com/" target="_blank">Multicoreware Inc</a> in the Autonomous and Automotive business unit. I gained valuable experience in software development for autonomous systems during my time there.
                </p>

                <p>
                    My passion for robotics began during my undergraduate studies at <a href="https://amrita.edu/" target="_blank">Amrita University</a>, where I majored in Electronics and Communications. I spent three years working as a student researcher at <a href="https://www.amrita.edu/center/humanitarian-technology-hut-labs/" target="_blank">Humanitarian Technology Labs (HuT Labs) </a>, where I worked on various robotics projects, published research papers, and participated in several competitions.
                </p>

                <p>
                    I am excited to continue my academic journey in the field of robotics and make meaningful contributions to the development of autonomous systems. In the future, I hope to explore new and innovative approaches to perception-based robotics and help shape the future of this rapidly-evolving field.
                </p>
                
                <p>For any collaborations, feel free to reach out to me!</p>
                <p>
                    <a href="https://drive.google.com/file/d/1jQiBqmjmihSH5HeMIQfdEpsj4_dhZ-F1/view?usp=sharing" target="_blank" style="margin-right: 15px"><i class="fa fa-address-card fa-lg"></i> Resume</a>
                    <a href="mailto:nigamkatta@gmail.com, nkatta9@gatech.edu" style="margin-right: 15px"><i class="far fa-envelope-open fa-lg"></i> Mail</a>
                    <a href="https://scholar.google.com/citations?user=WAjjOOwAAAAJ&hl=en" target="_blank" style="margin-right: 15px"><i class="fa-solid fa-book"></i> Scholar</a>
                    <a href="https://github.com/NigamKatta" target="_blank" style="margin-right: 15px"><i class="fab fa-github fa-lg"></i> Github</a>
                    <a href="https://www.linkedin.com/in/nigamkatta/" target="_blank" style="margin-right: 15px"><i class="fab fa-linkedin fa-lg"></i> LinkedIn</a>
                </p>
    
            </div>
            <div class="col-md-4" style="">
                <img src="assets/img/NigamKatta.jpeg" class="img-thumbnail" width="350px" alt="Profile picture">
            </div>
        </div>



        <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="">
                <h1>Professinal Experience</h1>
                <h2>Pathwave Measurement Analytics Intern</h2>
                <div class="date">May 2023 - August 2023</div>
                <div class="school">Keysight Labs, Atlanta, USA🇺🇸</div>
                <ul>
                    <li>Enhanced Keysight PathWave Measurement Analytics backend, optimizing ML-based decision-making processes, resulting in improved efficiency and performance.</li>
                    <li>Implemented a CPK Process Capability Index feature that enables ML-based statistical analysis for user-input data. This addition significantly enhanced the platform’s adaptability, leading to heightened client interest.</li>
                    <li>Played a pivotal role in the complex Angular-based frontend development, seamlessly integrating the CPK report UI component, contributing to increased user accessibility and platform usability.</li>                    
                </ul>

                <h2>Software Engineer (ADAS)</h2>
                <div class="date">October 2020 - December 2022</div>
                <div class="school">MulticoreWare Inc, Chennai, India 🇮🇳</div>
                <ul>
                    <li>Designed, Developed and Deployed end-to-end neural networks for perception in ADAS, including Object Detection, and Trajectory Prediction. Managed real world custom Datasets, which led to 25% Accuracy boost through transfer learning. </li>
                    <li>Build CNN architectures, including YOLO, U-Net, and Resnet, using Vectorization, SIMD techniques, for creating a Optimized CNN library for CEVA DSP. Achieved 50% FPS increase while maintaining the accuracy. </li>
                    li>Converted Deployed ML models into API's tp facilitate reusability across multiple projects.</li>
                    <li>Build an end-to-end Multiple Object tracker in Matlab using EKF, JPDA and clustering techniques such as DBSCAN. Applied Sensor fusion techniques to fuse Lidar and Radar Data to create robust sensing of the scene. Achieved 92% real-time acuracy.</li> 
                </ul>
                
                <h2 class="h2_nik">Student Researcher</h2>
                <div class="date">December 2017 - August 2020</div>
                <div class="school">Humanitarian Technology Labs in Amrita University, Kerala, India 🇮🇳</div>
                <ul>
                    <li>Worked on the software stack for the robotics, which includes Auto-Navigation, Speech Recognition and Computer Vision. Specifically, focused on designing and developing the Computer Vision interface to assist the robotic arm and also while performing auto-navigation.</li>
                    <li>Implemented Computer Vision algorithms to detect and recognize objects and obstacles in the robot's environment, allowing the robot to navigate safely and efficiently.</li>
                    <li>Configured and implemented mapping, localization, and pose estimation algorithms using sensor fusion techniques for outdoor and indoor environments. Worked on integrating GPS, LIDAR, and IMU data to create accurate maps and determine the robot's location and orientation in real-time.</li>
                    <li>Collaborated with cross-functional teams to deliver high-quality software products, including integrating the robot's software with hardware components such as motors and sensors.</li>
                    <li id="skills">Contributed to research and development of new robotics technologies, including exploring machine learning and AI techniques for object recognition and autonomous decision-making.</li>
                </ul>
            </div>
        </div>

        <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="">
                <h1>Skills</h1>

                <div class="row">
                <div class="col-md-6">
                  <ul class="skills-list">
                    <li class="li_nik">Python</li>
                    <li class="li_nik">C++</li>
                    <li class="li_nik">Deep Learning Techniques</li>
                    <li class="li_nik">Machine Learning</li>
                    <li class="li_nik">Gazebo</li>
                    <li class="li_nik" id="projects">Embedded Systems</li>
                  </ul>
                </div>
                <div class="col-md-6">
                  <ul class="skills-list">
                    <li class="li_nik">Computer Vision</li>
                    <li class="li_nik">SLAM (Simultaneous Localization and Mapping)</li>
                    <li class="li_nik">PyTorch</li>
                    <li class="li_nik">Sensor Fusion</li>
                    <li class="li_nik">TensorFlow</li>
                    <li class="li_nik">ROS (Robot Operating System)</li>
                  </ul>
                </div>
              </div>
            </div>
        </div>







        <div class="row" style="margin-top: 3em;">
                    <div class="col-sm-15" style="">
                        <h1>Projects</h1>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/attitude_estimation.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/attitude-estimation-matlab-iphone" target="_blank">Attitude Estimation of iPhone using MATLAB </a><br>This project focuses on the development of a MATLAB-based Attitude Estimation system, designed to accurately determine the orientation of a mobile device using data from its accelerometer and gyroscope sensors. The code implements various estimation techniques, including accelerometer-only estimation, gyroscope-only estimation, a complementary filter, and a Kalman filter. It processes sensor data and provides estimates of roll, pitch, and yaw angles, enabling a comprehensive understanding of the device's orientation in 3D space. <br><a href="https://github.com/GutlapalliNikhil/attitude-estimation-matlab-iphone" target="_blank">Code</a></div> </div> </div>
			    
			<div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/3d_map_2d_lidar.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/3D-Mapping-Using-2D-LiDAR-ROS" target="_blank">Simulating and Mapping 3D Environments with 2D Lidar in ROS </a><br>In this project, we harnessed the synergy of 2D Lidar, ROS, and Gazebo to pioneer 3D mapping. Our journey began with crafting a dynamic robot in SolidWorks, boasting a Lidar poised to scan from 0 to +60 degrees. A bespoke Lidar plugin orchestrated data collection, translating laser scans into intricate point clouds. Augmented by the octomap library, these clouds unfurled into vivid 3D maps. This venture not only showcases technical finesse but also exemplifies the potential of autonomous spatial understanding.<br><a href="https://github.com/GutlapalliNikhil/3D-Mapping-Using-2D-LiDAR-ROS" target="_blank">Code</a></div> </div> </div>
			    
                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/3D_detection.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/Complex-YOLO-ROS-3D-Object-Detection" target="_blank">Complex YOLO-ROS 3D Object-Detection </a><br>The Complex YOLO ROS 3D Object Detection project is an integration of the Complex YOLOv4 package into the ROS (Robot Operating System) platform, aimed at enhancing real-time perception capabilities for robotics applications. Using 3D object detection techniques based on Lidar data, the project enables robots and autonomous systems to accurately detect and localize objects in a 3D environment, crucial for safe navigation, obstacle avoidance, and intelligent decision-making.<br><a href="https://github.com/GutlapalliNikhil/Complex-YOLO-ROS-3D-Object-Detection" target="_blank">Code</a></div> </div> </div>
			    
                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/pointcloud_Classification.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/Pointcloud-Classification-Pytorch" target="_blank">Point Cloud Classification using PyTorch. </a><br>The Point Cloud Classification project implements the PointNet architecture for classifying 3D point clouds. It supports two datasets, ModelNet and ScanObjectNN, and provides functions for dataset preprocessing, model training, and visualization. The project aims to classify point clouds into different categories using deep learning techniques, enabling applications in object recognition, scene understanding, and robotics. The command-line interface allows easy configuration and training of the model.<br><a href="https://github.com/GutlapalliNikhil/Pointcloud-Classification-Pytorch" target="_blank">Code</a></div> </div> </div>

			<div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/object_detection.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/EmotionDetectAI" target="_blank">Real-time Emotion Detection on Human Faces using YOLOv5 and Android Deployment.</a><br>In my object detection project, I utilized the pre-trained YOLOv5 model from the Ultralytics repository to detect and classify emotions on human faces. Using the open-source software "labelme," I annotated images to create labeled datasets for training and validation. During the training phase, I fine-tuned the YOLOv5 model, optimizing its parameters to accurately identify "Happy" or "Neutral" expressions. Achieving high accuracy with the trained model, I then deployed the weights on an Android phone, developing a custom application for real-time emotion detection. This project showcases the practical application of object detection techniques for emotion recognition, with the ability to deploy such a solution on mobile devices.<br><a href="https://github.com/GutlapalliNikhil/EmotionDetectAI" target="_blank">Code</a></div> </div> </div>
			    
                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/image_classification.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/ImageClassification_VIT_TransferLearning" target="_blank">Custom Image Classification using Pretrained Transformer Model (ViT)</a><br>For my custom image classification project, I utilized a pretrained transformer model to achieve accurate results. By fine-tuning the model during the training phase and evaluating its performance during validation, I successfully adapted it to my specific image classification task. The pretrained transformer model's ability to capture intricate features and patterns in images proved highly effective in achieving high classification accuracy.<br><a href="https://github.com/GutlapalliNikhil/ImageClassification_VIT_TransferLearning" target="_blank">Code</a></div> </div> </div>

			<div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/collaborative_SLAM.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Group4_FinalReport-3.pdf" target="_blank">Collaborative SLAM using Multi-Robot System</a><br>Developed a novel approach to collaborative Simultaneous Localization and Mapping (SLAM) by utilizing a multi-robot system. The experiment employed two turtlebot3 robots operating in a simulated gazebo environment, with ROS serving as the platform for seamless integration. Leveraging the gmapping algorithm, we achieved highly precise mapping of the environment. Additionally, we equipped each robot with autonomous exploration capabilities using explore lite. The maps generated by the robots were skillfully merged using the multirobot map merger, resulting in a comprehensive and detailed representation of the environment.<br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Group4_FinalReport-3.pdf" target="_blank">Project Report</a> / <a href="https://github.com/GutlapalliNikhil/Collaborative_SLAM" target="_blank">Code</a> / <a href="https://www.youtube.com/watch?v=Mosx_JfMLIQ" target="_blank">Demo Video</a> / <a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Collaborative_SLAM.pdf" target="_blank">Slides</a></div> </div> </div>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/sTETRO.gif" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/NigamKatta/sTETRO/tree/main" target="_blank">sTetro: Revolutionizing Staircase-Cleaning Robotics</a><br> I played a key role in making sTetro, a cool robot that's changing how cleaning gets done. I focused on creating its brain - the staircase motion controller. It's smart and uses sensors to make decisions while moving around. I also made sure it can handle different surfaces smoothly by setting up the encoder and IMU-based PID controller. Then, I used fancy tech like Deep CNN and YOLO-v3 to help sTetro spot stairs accurately and climb them by itself. These cool tricks were added into sTetro's software, making it a super-efficient cleaner. Check out my work on GitHub to see how sTetro is shaking up the cleaning game!"<br></div> </div> </div>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/21.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/project.pdf" target="_blank">Collaborative Robotics on Navigational Platform</a><br>This project involves the development of an autonomous wheelchair with an integrated voice and robotic arm. The system aims to perform Human-Robot-Interaction and Cooperation, Navigation and Mapping in dynamic environments, Opening a Door, and Exchanging objects with a Companion. To achieve this, the team developed their own library for navigation and studied the wheelchair dynamics combined with a robotic arm. The system is validated using hardware and can be controlled either through speech commands or a mobile app. The team used ROS, Gmapping, Navigation Stack, and AMCL for navigation, a custom planner named "HuT Planner" for path planning, and a Markov model for speech recognition. They also created an app for speech recognition and utilized kinova software for the robotic arm. The project can find applications in various settings, including airports, households, and shopping malls, to assist physically disabled and elderly individuals in their daily activities.<br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/project.pdf" target="_blank">Thesis</a> / <a href="https://github.com/GutlapalliNikhil/Install_Speech_Navigation" target="_blank">Code</a> / <a href="https://www.amrita.edu/project/self-e/" target="_blank">More Info </a></div> </div> </div>

                        <div style="margin-bottom: 3em; margin-top: 1em;"> <div class="row"><div class="col-sm-5"><img src="assets/img/publications/chetak.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-7"><b>CHETAK 🐎 - The Home Service Robot</b><br>CHETAK is a self-governing home assistance robot designed to support individuals with disabilities and impairments with their daily life activities. It features an advanced and cost-effective infrastructure with Object Vision, Speech Recognition, Autonomous Navigation with Obstacle Avoidance, and a 6 Degree of Freedom Robotic Arm, making it a sophisticated service robot. The robot can differentiate among objects based on class and can anticipate qualities like Gender, Age, and Posture. It can receive commands from a person and perform tasks autonomously, either through manual or autonomous navigation, choosing the shortest path while avoiding dynamic obstacles and navigating through extended waypoints. CHETAK uses the robotic arm with end-effector and vision information to pick and place objects. The whole system is implemented using Robot Operating System(ROS) on Ubuntu platform to integrate individual nodes for complex operations. CHETAK can perform Human-Robot interaction, Object Manipulation, and Gesture recognition, helping the disabled and physically handicapped people by serving drinks, fruits, etc., and visually impaired people by finding objects or people and serving them.<br><a href="https://github.com/GutlapalliNikhil/Install_Speech_Navigation" target="_blank">Code</a> / <a href="https://amrita.edu/project/service-robot" target="_blank">More Info </a> / <a href="https://www.youtube.com/watch?v=1iNSb_AOZIM" target="_blank"> Demo Video - 1</a> / <a href="https://www.youtube.com/watch?v=L7wPotpw4lE" target="_blank" id="publications"> Demo Video - 2</a></div> </div> </div>

            </div>
        </div>

        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 2em">Publications</h1>
                <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/system_arch.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://github.com/NigamKatta/NigamKatta.github.io/assets/imgs/publications/Multimode_Control_and_Simulation_of_6_DO.pdf" target="_blank">Robot Operating System based Autonomous Navigation Platform with Human-Robot Interaction</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://www.linkedin.com/in/ravitejageesala/?originalSubdomain=in" target="_blank">Raviteja Geesala</a>, <a href="https://www.linkedin.com/in/ruthvikchanda/" target="_blank">Ruthvik Rangaiagh Chanda</a>, <a href="https://www.linkedin.com/in/nigamkatta/" target="_blank">Nigam Katta</a>,  <br><span style="font-style: italic;">Advances in Science, Technology and Engineering Systems Journal (ASTESJ)</span>, 2020 <br><a href="https://github.com/NigamKatta/NigamKatta.github.io/assets/imgs/publications/Multimode_Control_and_Simulation_of_6_DO.pdf" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#1" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="1"><div class="card card-body"><pre><code>@InProceedings{author = {Rajesh Kannan Megalingam, Ravi Teja Geesala, Ruthvik Rangaiah Chanda, Nigam Katta}, 
            title = {Multimode Control and Simulation of 6-DOF Robotic Arm in ROS}, 
            booktitle = {Advances in Science, Technology and Engineering Systems Journal (ASTESJ)}, 
            year = {2020}, 
        }</pre></code></div></div> </div> </div> </div>


            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/p2ss.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Integration%20of%20Vision%20based%20Robot%20Manipulation%20using%20ROS%20for%20Assistive%20Applications.pdf" target="_blank">Implementation of low-cost mobile robot for rescue challenges</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="" target="_blank">Shree Rajesh Raagul Vadivel</a>, <a href="" target="_blank">Prasant Kumar Yadav</a>, <a href="https://in.linkedin.com/in/nigam-katta" target="_blank">Nigam Katta</a>, <a href="https://www.linkedin.com/in/ravitejageesala/?originalSubdomain=in" target="_blank">Raviteja Geesala</a>, <a href="https://www.linkedin.com/in/ruthvikchanda/" target="_blank">Ruthvik Rangaiagh Chanda</a> <br><span style="font-style: italic;">Inventive Communication and Computational Technologies </span>, 2020 <br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Integration%20of%20Vision%20based%20Robot%20Manipulation%20using%20ROS%20for%20Assistive%20Applications.pdf" target="_blank">Paper</a> /<button class="btn btn-link" type="button" data-toggle="collapse" data-target="#2" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="2"><div class="card card-body"><pre><code>@INPROCEEDINGS{9183013,author={R. K. Megalingam, S. R. R. Vadivel, P. K. Yadav, K. Nigam, R. T. Geesala, R. Chanda},
              booktitle={2020 Fourth International Conference on Inventive Systems and Control (ICISC)}, 
              title={Implementation of low-cost mobile robot for rescue challenges}, 
              year={2020},
        }</pre></code></div></div> </div> </div> </div>


            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/p3ss.gif class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Human_Robot_Interaction_on_Navigation_platform_using_Robot_Operating_System.pdf" target="_blank">PID based locomotion of multi-terrain robot using ROS platform</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://nl.linkedin.com/in/motherammanaswini/" target="_blank">Deepak Nagalla</a>, <a href="https://in.linkedin.com/in/jahnavi-yannam-122606168" target="_blank">Nigam Katta</a>, <a href="https://in.linkedin.com/in/vignesh-s-naick-465747166" target="_blank">Vamsi Gontu</a>, <span style="font-weight: bold";>Phanindra Kumar Allada</span> <br><span style="font-style: italic;"> 2020 Fourth International Conference on Inventive Systems and Control (ICISC)</span>, 2020 <br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Human_Robot_Interaction_on_Navigation_platform_using_Robot_Operating_System.pdf" target="_blank">Paper</a> / <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#3" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="3"><div class="card card-body"><pre><code>@INPROCEEDINGS{9171065, author={Megalingam, Rajesh Kannan and Manaswini, Deepak Nagalla, Nigam Katta, Vamsi Gontu, Phanindra Kumar Allada},
              booktitle={2020 Fourth International Conference on Inventive Systems and Control (ICISC)}, 
              title={PID based locomotion of multi-terrain robot using ROS platform}, 
              year={2020},
            }</pre></code></div></div> </div> </div> </div>

            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/55.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Design_and_Implementation_of_an_Arena_for_Testing_and_Evaluating_Quadcopter.pdf" target="_blank">Keyboard-based control and simulation of 6-DOF robotic arm using ROS</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://in.linkedin.com/in/rohith-raj-rv-87229b148" target="_blank">Rokkam Venu Rohith Raj</a>, <a href="https://in.linkedin.com/in/akhil-masetti-0874b8167" target="_blank">Akhil Masetti</a>, <a href="https://www.linkedin.com/in/akhil-tammana" target="_blank">Tammana Akhil</a>, <span style="font-weight: bold";>Nikhil Chowdary Gutlapalli</span>, <a href="https://in.linkedin.com/in/vignesh-s-naick-465747166" target="_blank">Vignesh S. Naick</a><br><span style="font-style: italic;">2018 4th International Conference on Computing Communication and Automation (ICCCA)</span>, 2018 <br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Design_and_Implementation_of_an_Arena_for_Testing_and_Evaluating_Quadcopter.pdf" target="_blank" id="competitions">Paper</a> / <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#4" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="4"><div class="card card-body"><pre><code>@INPROCEEDINGS{8777578,  author={Rajesh Kannan Megalingam, Nigam Katta, Raviteja Geesala, Prasant Kumar Yadav, Ruthvik Chanda Rangaiah},
              booktitle={2018 4th International Conference on Computing Communication and Automation (ICCCA)}, 
              title={Keyboard-based control and simulation of 6-DOF robotic arm using ROS}, 
              year={2017},
            }</pre></code></div></div> </div> </div> </div>

            <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/55.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Design_and_Implementation_of_an_Arena_for_Testing_and_Evaluating_Quadcopter.pdf" target="_blank">Swarm based autonomous landmine detecting robots</a> <br><a href="https://www.amrita.edu/faculty/rajeshm/" target="_blank">Rajesh Kannan Megalingam</a>, <a href="https://in.linkedin.com/in/rohith-raj-rv-87229b148" target="_blank">Rokkam Venu Rohith Raj</a>, <a href="https://in.linkedin.com/in/akhil-masetti-0874b8167" target="_blank">Akhil Masetti</a>, <a href="https://www.linkedin.com/in/akhil-tammana" target="_blank">Tammana Akhil</a>, <span style="font-weight: bold";>Nikhil Chowdary Gutlapalli</span>, <a href="https://in.linkedin.com/in/vignesh-s-naick-465747166" target="_blank">Vignesh S. Naick</a><br><span style="font-style: italic;">2017 International Conference on Inventive Computing and Informatics (ICICI)</span>, 2018 <br><a href="https://github.com/GutlapalliNikhil/GutlapalliNikhil.github.io/blob/main/files/Design_and_Implementation_of_an_Arena_for_Testing_and_Evaluating_Quadcopter.pdf" target="_blank" id="competitions">Paper</a> / <button class="btn btn-link" type="button" data-toggle="collapse" data-target="#4" aria-expanded="false" aria-controls="collapseExample" style="margin-left: -6px; margin-top: -2px;">Expand bibtex</button><div class="collapse" id="4"><div class="card card-body"><pre><code>@INPROCEEDINGS{8777578,  author={Rajesh Kannan Megalingam, Deepak Nagalla, Pasumarthi Ravi Kiran, Ravi Teja Geesala, Katta Nigam},
              booktitle={2017 International Conference on Inventive Computing and Informatics (ICICI)}, 
              title={Swarm based autonomous landmine detecting robots}, 
              year={2017},
            }</pre></code></div></div> </div> </div> </div>

            </div>
        </div>






















        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 1.5em">Competitions</h1>
                      <ul class="competitions__list">
                        <li class="competitions__item">
                          <h3 class="competitions__title">DATATHON - Community Code Competition - Boston 2023 🇺🇸</h3>
                          <p class="competitions__description">Participated in the Kaggle Community Code Competition, a data science competition that was organized by Northeastern University to challeng participants to develop algorithms for predicting whether a photograph is likely to be popular and generate a significant number of downloads.identifying and tracking cells in microscope images.</p>
                          <p class="competitions__result_red">Ranked in top 30% of participants</p>
                        </li>

                        <li class="competitions__item">
                          <a href="https://www.amrita.edu/news/amrita-student-team-qualify-indy-autonomous-challenge-round-two/" target="_blank"><h3 class="competitions__title">Indy Autonomous Challenge - Indianapolis 2020 🇺🇸</h3></a>
                          <p class="competitions__description">Organized by Energy Systems Network, IAC university teams from around the world compete in a series of challenges to advance technology that can speed the commercialization of fully autonomous vehicles and deployments of advanced driver-assistance systems (ADAS) to increase safety and performance.</p>
                          <p class="competitions__result_red">Qualified for the 2nd Round.</p>
                        </li>


                        <li class="competitions__item">
                          <a href="https://www.amrita.edu/news/amrita-teams-win-225-million-japanese-yen-towards-world-robot-summit-2020/" target="_blank"><h3 class="competitions__title">World Robot Summit - Japan 2020 🇯🇵</h3></a>
                          <p class="competitions__description">The World Robot Summit (WRS) is Challenge and Expo of its kind to bring together Robot Excellence from around the world. Out Of 119 professional teams that applied from all over the world, We are the only team from India to qualify for the finals and took part in the "Future Convenience Store Challenge". </p>
                          <p class="competitions__result_red">Won 1 million JPY Cash Price.</p>
                        </li>


                        <li class="competitions__item">
                          <a href=https://www.amrita.edu/news/hut-labs-student-teams-participate-robocup-german-open-2019/ target="_blank"><h3 class="competitions__title">RoboCup@Home - Germany 2019 🇩🇪</h3></a>
                          <p class="competitions__description">The RoboCup@Home league aims to develop service and assistive robot technology with high relevance for future personal domestic applications. It is the largest international annual competition for autonomous service robots. A set of benchmark tests is used to evaluate the robots’ abilities and performance in a realistic non-standardized home environment setting.</p>
                          <p class="competitions__result_red" id="awards">Qualified for the 2nd Round.</p>
                        </li>
                      </ul>
            </div>
        </div>










        <div class="row" style="margin-top: 1em;">
            <div class="col-sm-12" style="">
                <h1 style="margin-bottom: 1.5em; margin-top: 2em;">Awards</h1>
                      <ul class="competitions__list">

                        <li class="competitions__item">
                          <h4 class="competitions__title_award">Aspiring Minds Motivational Award 2020 - Dept of ECE, Amrita University.</h4>
                        </li>

                        <li class="competitions__item">
                          <h4 class="competitions__title_award">Outstanding Student Reseacher Award 2019 - HuT Labs.</h4>
                        </li>

                        <li class="competitions__item">
                          <h4 class="competitions__title_award">Certificate of Excellence 2019 - 2020 by IEEE Student Branch.</h4>
                        </li>
                      </ul>
            </div>
        </div>

        <div class="row" style="margin-top: 3em; margin-bottom: 1em;">
            
            <div class="col-sm-12" style="">
                <h4 class="end">Website template provided by <a href=https://m-niemeyer.github.io/ target="_blank">Michael Niemeyer</a>. Check out his <a href=https://github.com/m-niemeyer/m-niemeyer.github.io target="_blank">github repository </a> for instructions on how to use it! <h4>
            </div>
    
        </div>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
      integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
      crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
      integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
      crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
      integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
      crossorigin="anonymous"></script>
</body>

</html>
    
